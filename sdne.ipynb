{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdne.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1BXahrdGm0y7fiIDNHgOVUgYUU3vU4k17",
      "authorship_tag": "ABX9TyMNI3Y+DhhHpGpqqbyMCe1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaZzyMan/Notebook/blob/master/sdne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4pWqwnSlLSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "import pickle\n",
        "import os\n",
        "import numpy\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Model, backend as K, regularizers\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, Multiply, Subtract, Lambda, BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from joblib import Parallel, delayed\n",
        "import psycopg2\n",
        "import geopandas as gpd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9KfolWXn4eN",
        "colab_type": "text"
      },
      "source": [
        "# sdne-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw0AVh4VlZ7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_nxgraph(graph):\n",
        "    # table from index to node and from node to index\n",
        "    node2idx = {}\n",
        "    idx2node = []\n",
        "    node_size = 0\n",
        "    for node in graph.nodes():\n",
        "        node2idx[node] = node_size\n",
        "        idx2node.append(node)\n",
        "        node_size += 1\n",
        "    return idx2node, node2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwcFAdnMlaaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_reconstruction_loss(beta):\n",
        "    # 二阶相似度损失函数\n",
        "    def reconstruction_loss(true_y, pred_y):\n",
        "        # diff = K.abs(true_y - pred_y)\n",
        "        # y = K.cast(true_y, 'bool')\n",
        "        # y = K.cast(y, 'int8')\n",
        "        # y = K.cast(y, 'float32')\n",
        "        # weight = y * (beta - 1) + 1\n",
        "        # weighted_diff = diff * weight\n",
        "        # 改用pearson相似度，消除尺度影响\n",
        "        # true_y_nore = tf.norm(true_y, axis=-1, keepdims=False)\n",
        "        # pred_y_norm = tf.norm(pred_y, axis=-1, keepdims=False)\n",
        "        # p = 1. - K.sum(true_y * pred_y, axis=-1) / (true_y_nore * pred_y_norm + 1e-6)\n",
        "        # 弱化权重影响的曼哈顿距离，同时惩罚稀疏预测\n",
        "        y_max = K.maximum(true_y, pred_y) + .1\n",
        "        y_min = K.minimum(true_y, pred_y) + .1\n",
        "        return K.mean(K.mean((y_max - y_min) / y_min, axis=-1))\n",
        "\n",
        "    return reconstruction_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLOPAD_O7XOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edge_wise_loss(true_y, embedding_diff):\n",
        "    # 一阶相似度损失函数\n",
        "    # return K.mean(K.sum(K.square(embedding_diff), axis=1))\n",
        "    return K.mean(K.sum(embedding_diff, axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biewiX9UlaXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SDNE():\n",
        "    def __init__(self,\n",
        "                 graph,\n",
        "                 encode_dim,\n",
        "                 encoding_layer_dims,\n",
        "                 weight='weight',\n",
        "                 beta=2, alpha=2, theta=2,\n",
        "                 l2_param=1):\n",
        "        self.encode_dim = encode_dim\n",
        "        self.graph = graph\n",
        "        self.idx2node, self.node2idx = preprocess_nxgraph(self.graph)\n",
        "        self.N = graph.number_of_nodes()\n",
        "        self.adj_mat = nx.adjacency_matrix(self.graph).toarray()\n",
        "        self.edges = np.array(list(self.graph.edges()))\n",
        "        weights = [graph[u][v].get(weight, 1.0)\n",
        "                   for u, v in self.graph.edges()]\n",
        "        self.weights = np.array(weights, dtype=np.float32)[:, None]\n",
        "\n",
        "        # input_a = Input(shape=(self.N,), name='input-a', dtype='float32')\n",
        "        # input_b = Input(shape=(self.N,), name='input-b', dtype='float32')\n",
        "\n",
        "        input_a = Input(shape=(1,), name='input-a', dtype='int32')\n",
        "        input_b = Input(shape=(1,), name='input-b', dtype='int32')\n",
        "        edge_weight = Input(shape=(1,), name='edge_weight', dtype='float32')\n",
        "\n",
        "        encoding_layers = []\n",
        "        decoding_layers = []\n",
        "        embedding_layer = Embedding(output_dim=self.N, input_dim=self.N,\n",
        "                                    trainable=False, input_length=1, name='nbr-table')\n",
        "        embedding_layer.build((None,))\n",
        "        embedding_layer.set_weights([self.adj_mat])\n",
        "\n",
        "        encoding_layers.append(embedding_layer)\n",
        "        encoding_layers.append(Lambda(lambda x: tf.reduce_sum(x, axis=1, keepdims=False), name='reduce-dim'))\n",
        "\n",
        "        # encoding\n",
        "        encoding_layer_dims += [encode_dim]\n",
        "        for i, dim in enumerate(encoding_layer_dims):\n",
        "            layer = Dense(dim, activation=LeakyReLU(alpha=.3),\n",
        "                          kernel_regularizer=regularizers.l2(l2_param),\n",
        "                          name='encoding-layer-{}'.format(i))\n",
        "            encoding_layers.append(layer)\n",
        "            # bn = BatchNormalization()\n",
        "            # encoding_layers.append(bn)\n",
        "\n",
        "        # decoding\n",
        "        decoding_layer_dims = encoding_layer_dims[::-1][1:] + [self.N]\n",
        "        for i, dim in enumerate(decoding_layer_dims):\n",
        "            if i == len(decoding_layer_dims) - 1:\n",
        "                activation = 'relu'\n",
        "                # activation = LeakyReLU(alpha=.3)\n",
        "            else:\n",
        "                activation = 'relu'\n",
        "                # activation = LeakyReLU(alpha=.3)\n",
        "            layer = Dense(\n",
        "                dim, activation=activation,\n",
        "                kernel_regularizer=regularizers.l2(l2_param),\n",
        "                name='decoding-layer-{}'.format(i))\n",
        "            decoding_layers.append(layer)\n",
        "            # bn = BatchNormalization()\n",
        "            # decoding_layers.append(bn)\n",
        "        all_layers = encoding_layers + decoding_layers\n",
        "\n",
        "        encoded_a = reduce(lambda arg, f: f(arg), encoding_layers, input_a)\n",
        "        encoded_b = reduce(lambda arg, f: f(arg), encoding_layers, input_b)\n",
        "        decoded_a = reduce(lambda arg, f: f(arg), all_layers, input_a)\n",
        "        decoded_b = reduce(lambda arg, f: f(arg), all_layers, input_b)\n",
        "        \n",
        "        # embedding_diff = Subtract()([encoded_a, encoded_b])\n",
        "        # 一阶相似度损失函数从LE改为KL散度省略常数项 -w*log(sigmoid(yi*yj))\n",
        "        embedding_diff = Multiply()([encoded_a, encoded_b])\n",
        "        embedding_diff = Lambda(lambda x: - K.log(K.sigmoid(tf.reduce_sum(x, axis=-1, keepdims=True))))(embedding_diff)\n",
        "        embedding_diff = Multiply(name='1st')([embedding_diff, edge_weight])\n",
        "\n",
        "        self.model = Model([input_a, input_b, edge_weight], [decoded_a, decoded_b, embedding_diff])\n",
        "\n",
        "        reconstruction_loss = build_reconstruction_loss(beta)\n",
        "\n",
        "        self.model.compile(optimizer=optimizers.Adam(),\n",
        "                           loss=[reconstruction_loss, reconstruction_loss, edge_wise_loss],\n",
        "                           loss_weights=[theta, theta, alpha])\n",
        "\n",
        "        self.encoder = Model(input_a, encoded_a)\n",
        "\n",
        "        # for pre-training\n",
        "        self.auto_encoder = Model(input_a, decoded_a)\n",
        "        self.auto_encoder.compile(optimizer=optimizers.Adam(), loss=reconstruction_loss)\n",
        "\n",
        "        self._embeddings = {}\n",
        "\n",
        "    def pretrain(self, **kwargs):\n",
        "        \"\"\"pre-train the auto encoder without edges\"\"\"\n",
        "        nodes = np.arange(self.graph.number_of_nodes())\n",
        "        node_neighbors = self.adj_mat[nodes]\n",
        "\n",
        "        self.auto_encoder.fit(nodes[:, None],\n",
        "                              node_neighbors,\n",
        "                              shuffle=True,\n",
        "                              **kwargs)\n",
        "\n",
        "    def train_data_generator(self, batch_size=32):\n",
        "        # this can become quadratic if using dense\n",
        "        m = self.graph.number_of_edges()\n",
        "        shuffled_edges = np.random.permutation(self.edges)\n",
        "        while True:\n",
        "            for i in range(math.ceil(m / batch_size)):\n",
        "                sel = slice(i * batch_size, (i + 1) * batch_size)\n",
        "                nodes_a = shuffled_edges[sel, 0][:, None]\n",
        "                nodes_b = shuffled_edges[sel, 1][:, None]\n",
        "                weights = np.array([[self.graph.edges[edge].get('weight', 1.)] for edge in shuffled_edges[sel]])\n",
        "\n",
        "                nodes_a_idx = np.array([[self.node2idx[node]] for node in nodes_a.flatten()])\n",
        "                nodes_b_idx = np.array([[self.node2idx[node]] for node in nodes_b.flatten()])\n",
        "                neighbors_a = self.adj_mat[nodes_a_idx.flatten()]\n",
        "                neighbors_b = self.adj_mat[nodes_b_idx.flatten()]\n",
        "\n",
        "                # requires to have the same shape as embedding_diff\n",
        "                dummy_output = np.zeros((nodes_a.shape[0], self.encode_dim))\n",
        "\n",
        "                yield ([nodes_a_idx, nodes_b_idx, weights],\n",
        "                       [neighbors_a, neighbors_b, dummy_output])\n",
        "\n",
        "                # yield ([neighbors_a, neighbors_b, weights],\n",
        "                #        [neighbors_a, neighbors_b, dummy_output])\n",
        "\n",
        "    def fit(self, log=False, **kwargs):\n",
        "        \"\"\"kwargs: keyword arguments passed to `model.fit`\"\"\"\n",
        "        if log:\n",
        "            callbacks = [keras.callbacks.TensorBoard(\n",
        "                log_dir='./log', histogram_freq=0,\n",
        "                write_graph=True, write_images=False)]\n",
        "        else:\n",
        "            callbacks = []\n",
        "\n",
        "        callbacks += kwargs.get('callbacks', [])\n",
        "        if 'callbacks' in kwargs:\n",
        "            del kwargs['callbacks']\n",
        "\n",
        "        if 'batch_size' in kwargs:\n",
        "            batch_size = kwargs['batch_size']\n",
        "            del kwargs['batch_size']\n",
        "            gen = self.train_data_generator(batch_size=batch_size)\n",
        "        else:\n",
        "            gen = self.train_data_generator()\n",
        "\n",
        "        self.model.fit(\n",
        "            gen,\n",
        "            shuffle=True,\n",
        "            callbacks=callbacks,\n",
        "            **kwargs)\n",
        "\n",
        "    def get_node_embedding(self):\n",
        "        nodes = np.array([self.node2idx[node] for node in self.graph.nodes()])[:, None]\n",
        "        embeddings = self.encoder.predict(nodes)\n",
        "        for embedding, node in zip(embeddings, self.graph.nodes):\n",
        "            self._embeddings[node] = embedding\n",
        "        return self._embeddings\n",
        "\n",
        "    def save(self, path):\n",
        "        self.model.save(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pqCDsTSlaUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(graph, alpha, theta, l2_param, pretrain_epochs, epochs, beta, encoding_layer_dims, batch_size):\n",
        "    model = SDNE(graph,\n",
        "                 encode_dim=128,\n",
        "                 encoding_layer_dims=encoding_layer_dims,\n",
        "                 beta=beta,\n",
        "                 theta=theta,\n",
        "                 alpha=alpha,\n",
        "                 l2_param=l2_param)\n",
        "    model.pretrain(epochs=pretrain_epochs, batch_size=32)\n",
        "    n_batches = math.ceil(graph.number_of_edges() / batch_size)\n",
        "    model.fit(epochs=epochs, log=True, batch_size=batch_size, steps_per_epoch=n_batches)\n",
        "    embeddings = model.get_node_embedding()\n",
        "    with open('/content/drive/My Drive/Data/sdne_keras_embedding.pickle', 'wb') as f:\n",
        "        pickle.dump(embeddings, f)\n",
        "        f.close()\n",
        "    evaluate_embeddings(embeddings, labels, 'sdne_keras_eva')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkaAdiC9oBvq",
        "colab_type": "text"
      },
      "source": [
        "# evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPTFkp0YoJF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return numpy.asarray(all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LnpKoOHoJN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(object):\n",
        "    def __init__(self, embeddings, clf):\n",
        "        self.embeddings = embeddings\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        X_train = [self.embeddings[x] for x in X]\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        # top_k_list = [len(l) for l in Y]\n",
        "        top_k_list = [1] * len(Y)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        results['acc'] = accuracy_score(Y, Y_)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = numpy.asarray([self.embeddings[x] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent):\n",
        "        state = numpy.random.get_state()\n",
        "        training_size = int(train_precent * len(X))\n",
        "        # numpy.random.seed()\n",
        "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        self.train(X_train, Y_train, Y)\n",
        "        numpy.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSIeXUeQoJTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_embeddings(embeddings, labels, filename):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for node, label in labels.items():\n",
        "        X.append(node)\n",
        "        Y.append(label.strip())\n",
        "    # print(\"Training classifier using {:.2f}% nodes...\".format(tr_frac * 100))\n",
        "    clf = Classifier(embeddings=embeddings, clf=LogisticRegression(solver='liblinear'))\n",
        "    results = {'train_percent': [], 'micro-f1': [], 'macro-f1': [], 'acc': []}\n",
        "    for tr_frac in [.1, .2, .5, .8]:\n",
        "        result = clf.split_train_evaluate(X, Y, tr_frac)\n",
        "        results['train_percent'].append('%02d%%' % (tr_frac * 100))\n",
        "        results['micro-f1'].append(result['micro'])\n",
        "        results['macro-f1'].append(result['macro'])\n",
        "        results['acc'].append(result['acc'])\n",
        "    df = pd.DataFrame(results)\n",
        "    print(df)\n",
        "    df.to_csv('/content/drive/My Drive/Data/%s.csv' % filename)\n",
        "\n",
        "def f1_micro_percent_50(embeddings, labels):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for node, label in labels.items():\n",
        "        X.append(node)\n",
        "        Y.append(label.strip())\n",
        "    clf = Classifier(embeddings=embeddings, clf=LogisticRegression(solver='liblinear'))\n",
        "    result = clf.split_train_evaluate(X, Y, .5)\n",
        "    return result['micro']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp7MV2dhoLoN",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NULqrx1lafB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Data/graph_undirected_20151201.gpickle', 'rb') as f:\n",
        "  graph = pickle.load(f)\n",
        "  f.close()\n",
        "labels = pickle.load(open('/content/drive/My Drive/Data/label_100.pickle', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FaBdzRRpJOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "187b99da-3a13-468a-d05f-9849f227313f"
      },
      "source": [
        "train(graph, alpha=2., theta=1., l2_param=1e-3, pretrain_epochs=30, epochs=10, beta=3, encoding_layer_dims=[500, 200], batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 1.8630\n",
            "Epoch 2/30\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 0.9057\n",
            "Epoch 3/30\n",
            "48/48 [==============================] - 2s 34ms/step - loss: 0.6647\n",
            "Epoch 4/30\n",
            "48/48 [==============================] - 2s 34ms/step - loss: 0.5486\n",
            "Epoch 5/30\n",
            "48/48 [==============================] - 2s 33ms/step - loss: 0.4899\n",
            "Epoch 6/30\n",
            "48/48 [==============================] - 2s 34ms/step - loss: 0.4618\n",
            "Epoch 7/30\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 0.4424\n",
            "Epoch 8/30\n",
            "48/48 [==============================] - 1s 19ms/step - loss: 0.4443\n",
            "Epoch 9/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.4179\n",
            "Epoch 10/30\n",
            "48/48 [==============================] - 1s 19ms/step - loss: 0.4014\n",
            "Epoch 11/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3921\n",
            "Epoch 12/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3882\n",
            "Epoch 13/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3820\n",
            "Epoch 14/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3785\n",
            "Epoch 15/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3844\n",
            "Epoch 16/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3924\n",
            "Epoch 17/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3792\n",
            "Epoch 18/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3723\n",
            "Epoch 19/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3702\n",
            "Epoch 20/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3758\n",
            "Epoch 21/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3667\n",
            "Epoch 22/30\n",
            "48/48 [==============================] - 1s 19ms/step - loss: 0.3648\n",
            "Epoch 23/30\n",
            "48/48 [==============================] - 1s 19ms/step - loss: 0.3657\n",
            "Epoch 24/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3661\n",
            "Epoch 25/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3654\n",
            "Epoch 26/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3635\n",
            "Epoch 27/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3623\n",
            "Epoch 28/30\n",
            "48/48 [==============================] - 1s 19ms/step - loss: 0.3607\n",
            "Epoch 29/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3618\n",
            "Epoch 30/30\n",
            "48/48 [==============================] - 1s 20ms/step - loss: 0.3748\n",
            "Epoch 1/10\n",
            "1497/1497 [==============================] - 137s 92ms/step - loss: 1.0605 - decoding-layer-2_loss: 0.6615 - decoding-layer-2_1_loss: 0.3255 - 1st_loss: 4.7633e-07\n",
            "Epoch 2/10\n",
            "1497/1497 [==============================] - 135s 90ms/step - loss: 1.0233 - decoding-layer-2_loss: 0.6190 - decoding-layer-2_1_loss: 0.3085 - 1st_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1497/1497 [==============================] - 135s 90ms/step - loss: 1.0143 - decoding-layer-2_loss: 0.6085 - decoding-layer-2_1_loss: 0.3043 - 1st_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1497/1497 [==============================] - 135s 90ms/step - loss: 1.0087 - decoding-layer-2_loss: 0.6027 - decoding-layer-2_1_loss: 0.3018 - 1st_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1497/1497 [==============================] - 139s 93ms/step - loss: 1.0062 - decoding-layer-2_loss: 0.5995 - decoding-layer-2_1_loss: 0.3008 - 1st_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1497/1497 [==============================] - 135s 90ms/step - loss: 1.0025 - decoding-layer-2_loss: 0.5962 - decoding-layer-2_1_loss: 0.2992 - 1st_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1497/1497 [==============================] - 137s 92ms/step - loss: 1.0008 - decoding-layer-2_loss: 0.5942 - decoding-layer-2_1_loss: 0.2986 - 1st_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1497/1497 [==============================] - 140s 93ms/step - loss: 0.9991 - decoding-layer-2_loss: 0.5924 - decoding-layer-2_1_loss: 0.2978 - 1st_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1497/1497 [==============================] - 144s 96ms/step - loss: 0.9976 - decoding-layer-2_loss: 0.5910 - decoding-layer-2_1_loss: 0.2972 - 1st_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1497/1497 [==============================] - 140s 93ms/step - loss: 0.9965 - decoding-layer-2_loss: 0.5899 - decoding-layer-2_1_loss: 0.2967 - 1st_loss: 0.0000e+00\n",
            "  train_percent  micro-f1  macro-f1       acc\n",
            "0           10%  0.630466  0.415060  0.630466\n",
            "1           20%  0.665574  0.424167  0.665574\n",
            "2           50%  0.664042  0.425106  0.664042\n",
            "3           80%  0.662295  0.453306  0.662295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwGuexl8y5Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_with_param(param):\n",
        "  model = SDNE(graph, encode_dim=128, encoding_layer_dims=[200], beta=3, theta=param['theta'], alpha=param['alpha'], l2_param=param['l2_param'])\n",
        "  model.pretrain(epochs=10, batch_size=32)\n",
        "  n_batches = math.ceil(graph.number_of_edges() / param['batch_size'])\n",
        "  model.fit(epochs=10, log=True, batch_size=param['batch_size'], steps_per_epoch=n_batches)\n",
        "  embeddings = model.get_node_embedding()\n",
        "  score = f1_micro_percent_50(embeddings, labels)\n",
        "  result = {'params':param, 'score': score}\n",
        "  print(result)\n",
        "  return result\n",
        "param_dist = {'alpha': np.linspace(0.01, 1, 10),\n",
        "        'theta': np.linspace(0.01, 1, 10),\n",
        "        'l2_param': [1e-1, 1e-3, 1e-5],\n",
        "        'batch_size': [32, 64, 128, 256, 512, 1024]}\n",
        "param_grid = ParameterGrid(param_dist)\n",
        "results = Parallel(n_jobs=-1, verbose=1)(delayed(fit_with_param)(param) for param in param_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsfHCJMVAkkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e98b20e8-93bd-448e-8c29-433b70505147"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'params': {'alpha': 0.1,\n",
              "   'batch_size': 128,\n",
              "   'l2_param': 0.001,\n",
              "   'theta': 0.01},\n",
              "  'score': 0.6060037523452159},\n",
              " {'params': {'alpha': 0.2,\n",
              "   'batch_size': 128,\n",
              "   'l2_param': 0.001,\n",
              "   'theta': 0.01},\n",
              "  'score': 0.6067415730337079}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWue4VsI0V1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = pkl.load(open('/content/drive/My Drive/Data/sdne_keras_embedding.pkl', 'rb'))\n",
        "label = pkl.load(open('/content/drive/My Drive/Data/label.pickle', 'rb'))\n",
        "\n",
        "emb_df = pd.DataFrame(embeddings)\n",
        "emb_df.to_csv('/content/drive/My Drive/Data/sdne_keras_embedding.tsv', sep='\\t', header=False, index=False)\n",
        "\n",
        "embedding_var = tf.Variable(embeddings, name='node_embeddings')\n",
        "\n",
        "\n",
        "LOG_DIR = 'log'\n",
        "\n",
        "\n",
        "df = pd.Series(labels, name='label')\n",
        "df.to_frame().to_csv(LOG_DIR + '/node_labels.tsv', index=False, header=False)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver([embedding_var])\n",
        "\n",
        "  sess.run(embedding_var.initializer)\n",
        "  saver.save(sess, os.path.join(LOG_DIR, 'embeddings.ckpt'))\n",
        "    \n",
        "  config = projector.ProjectorConfig()\n",
        "  # One can add multiple embeddings.\n",
        "  embedding = config.embeddings.add()\n",
        "  embedding.tensor_name = embedding_var.name\n",
        "  # Link this tensor to its metadata file (e.g. labels).\n",
        "  embedding.metadata_path = 'node_labels.tsv'\n",
        "  # Saves a config file that TensorBoard will read during startup.\n",
        "    \n",
        "  projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}